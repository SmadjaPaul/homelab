---
date: 2026-01-31
project: homelab
version: 6.0
status: current
lastUpdated: 2026-01-31
note: v6.0 - Authentik as IdP; validation manuelle avant accÃ¨s apps; apps admin non exposÃ©es; service accounts; session travail _bmad-output/planning-artifacts/session-travail-authentik.md
---

# Architecture Document: Homelab Infrastructure with Proxmox + Omni

**Purpose**: This document defines the architectural decisions, implementation patterns, and project structure for a homelab infrastructure built on Proxmox VE with Talos Linux Kubernetes clusters managed by Omni, featuring Dev/Prod environments with CI/CD pipeline.

**Key Inspirations & What We Adopt**:

| Repo | Adopted Patterns | Services/Tools |
|------|------------------|----------------|
| [qjoly/GitOps](https://github.com/qjoly/GitOps) | Omni cluster templates, Talos config | Vault, Volsync, Cloudflare Tunnels |
| [ravilushqa/homelab](https://github.com/ravilushqa/homelab) | Proxmox + Terraform patterns | Gateway API, Home Assistant, Immich |
| [mitchross/talos-argocd-proxmox](https://github.com/mitchross/talos-argocd-proxmox) | ArgoCD sync waves, GPU Operator | Longhorn, sync wave architecture |
| [Mafyuh/iac](https://github.com/Mafyuh/iac) | Security stack, automation | Twingate, Wazuh, n8n, Bitwarden, Trivy |
| [ahinko/home-ops](https://github.com/ahinko/home-ops) | Renovate config, Taskfile | Automated updates, justfile patterns |

---

## 1. Project Context

### Hardware

**Homelab Server** (Proxmox Host):
- **IP**: 192.168.68.51 (Proxmox Web UI: https://192.168.68.51:8006)
- **RAM**: 64GB
- **Storage**: 1TB SSD (system), 2x 20TB HDD (data)
- **GPU**: NVIDIA GPU (for gaming VMs)

**Oracle Cloud** (Always Free Tier):
- **Instances**: 2 ARM VMs (12GB RAM + 2 OCPUs each = 24GB total)
- **Storage**: 139GB block storage
- **Bandwidth**: 10 TB/month egress

### Target Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      OMNI (Self-Hosted on OCI)      â”‚
                    â”‚  Single pane of glass for clusters  â”‚
                    â”‚  + Authentik SSO + Cloudflare Tunnel â”‚
                    â”‚  + Workload Proxy (auth services)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                            â”‚                            â”‚
           â–¼                            â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Proxmox (Home)    â”‚    â”‚   Oracle Cloud      â”‚    â”‚   CI / GitOps       â”‚
â”‚   PROD Cluster      â”‚    â”‚   CLOUD Cluster     â”‚    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚    â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚    â”‚   â€¢ Ephemeral DEV   â”‚
â”‚   â€¢ Stable services â”‚    â”‚   â€¢ Family services â”‚    â”‚     (KubeVirt in     â”‚
â”‚   â€¢ Gaming VMs      â”‚    â”‚   â€¢ External access â”‚    â”‚     CLOUD)          â”‚
â”‚   â€¢ Local storage   â”‚    â”‚   â€¢ KubeVirt host   â”‚    â”‚   â€¢ create â†’ test   â”‚
â”‚   â€¢ 16GB RAM        â”‚    â”‚     for ephemeral   â”‚    â”‚     â†’ destroy        â”‚
â”‚   â€¢ No DEV VM 24/7  â”‚    â”‚     DEV (CI)        â”‚    â”‚   â€¢ No DEV on Proxmoxâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                            â”‚                            â”‚
           â”‚                            â”‚â—„â”€â”€â”€â”€Twingate/WireGuardâ”€â”€â”€â”€â–ºâ”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                        â”‚
                               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                               â”‚   GitOps Repo   â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Requirements Summary

**Environments**:
- **DEV (Ã©phÃ©mÃ¨re)**: Cluster crÃ©Ã© Ã  la volÃ©e par la CI via KubeVirt + Omni, test des changements, destruction aprÃ¨s MEP si pas dâ€™erreur (pas de cluster DEV permanent sur Proxmox).
- **PROD**: Environnement de production sur Proxmox, reÃ§oit les dÃ©ploiements aprÃ¨s validation sur DEV Ã©phÃ©mÃ¨re.
- **CLOUD**: Cluster Oracle Cloud pour services famille et hÃ©bergement dâ€™Omni.

**Target Users**: Developer administrator (Paul), graphic designer, family members (5 people)

---

## 2. Core Architectural Decisions

### 2.1 Hypervisor: Proxmox VE

**Decision**: Proxmox VE as the base hypervisor with Infrastructure as Code

**Rationale**:
- âœ… Web-based VM management (https://192.168.68.51:8006)
- âœ… Terraform provider for IaC (`bpg/proxmox`)
- âœ… GPU passthrough support for gaming VMs
- âœ… ZFS support for storage
- âœ… Mature, stable platform

**IaC Tools**:
- **Terraform/OpenTofu**: VM provisioning, network configuration
- **Packer**: Talos Linux image templates (optional, can use ISO)
- **Ansible**: Initial Proxmox configuration, ZFS setup

---

### 2.2 Kubernetes OS: Talos Linux

**Decision**: Talos Linux for all Kubernetes nodes

**Rationale**:
- âœ… Immutable, API-only OS (no SSH, minimal attack surface)
- âœ… Kubernetes-optimized
- âœ… Atomic updates with rollback
- âœ… Minimal overhead (~200MB footprint)
- âœ… Native Omni integration

**Versions**:
- Talos: Latest stable (1.9.x)
- Kubernetes: Latest stable (1.32.x)

---

### 2.3 Cluster Management: Omni (Self-Hosted on Oracle Cloud)

**Decision**: Self-hosted Omni on Oracle Cloud ARM instance

**Rationale**:
- âœ… Single pane of glass for all clusters (Dev, Prod, Cloud)
- âœ… Declarative cluster configuration
- âœ… SSO authentication (integrated with Authentik)
- âœ… Secure kubeconfig distribution
- âœ… Cluster lifecycle management (upgrades, scaling)
- âœ… Multi-cluster visibility
- âœ… Free hosting on Oracle Cloud Always Free tier
- âœ… Accessible from anywhere (no home network exposure)

**Deployment**: Self-Hosted on Oracle Cloud ARM VM (2GB RAM, 1 OCPU)

**Components**:
- **Omni Server**: Main management interface
- **PostgreSQL**: Database for Omni state
- **Nginx**: HTTPS reverse proxy with Let's Encrypt
- **Authentik Integration**: SSO for all users (SAML ; [Integrate with Omni](https://integrations.goauthentik.io/infrastructure/omni/))

**Key Features**:
- **MachineClass**: Define node profiles (control-plane, worker, GPU worker)
- **ClusterTemplate**: Declarative cluster definitions
- **Infrastructure Provider**: Proxmox integration for auto-provisioning

**References**:
- [Deploy Omni On-Prem](https://omni.siderolabs.com/how-to-guides/self_hosted/)
- [Integrate Authentik with Omni](https://integrations.goauthentik.io/infrastructure/omni/)

---

### 2.3.1 Placement dâ€™Omni : VPC (OCI) vs local (Homelab)

**Contexte** : Omni peut Ãªtre hÃ©bergÃ© sur Oracle Cloud (VPC) ou en local sur le serveur Homelab. Les deux options ont des implications fortes sur la disponibilitÃ© et la surface dâ€™attaque.

| CritÃ¨re | Omni sur VPC (OCI) | Omni en local (Homelab) |
|--------|---------------------|--------------------------|
| **DisponibilitÃ©** | Omni toujours joignable (serveur OCI 24/7) | Omni down dÃ¨s que le serveur est Ã©teint |
| **Administration des clusters** | Possible mÃªme quand le serveur est Ã©teint (vacances, absence, Ã©conomie dâ€™Ã©nergie) | Impossible dâ€™administrer les clusters si le serveur est off |
| **Surface dâ€™attaque** | Omni exposÃ© via Cloudflare Tunnel â†’ plus de risque quâ€™en local | Omni uniquement sur rÃ©seau local â†’ moins exposÃ© |
| **Risque opÃ©rationnel** | Faible : pas de perte de capacitÃ© dâ€™administration | Ã‰levÃ© : serveur off = perte totale de contrÃ´le (talosctl, kubeconfig, upgrades) |

**Recommandation : Omni sur VPC (Oracle Cloud)**.

- **Raison principale** : Si Omni est down (serveur local Ã©teint), on perd la capacitÃ© dâ€™administration de *tous* les clusters (PROD Proxmox, CLOUD OCI). Impossible de faire des upgrades Talos, de rÃ©cupÃ©rer un kubeconfig, ou de diagnostiquer Ã  distance. Câ€™est un risque opÃ©rationnel majeur pour un homelab oÃ¹ le serveur est rÃ©guliÃ¨rement Ã©teint.
- **Mitigation du risque VPC** : Omni en VPC reste derriÃ¨re Cloudflare Tunnel (pas de port ouvert direct), avec authentification Authentik (SAML/OIDC). On peut durcir davantage : firewall OCI, IP allowlist Cloudflare, 2FA sur Authentik, audit logs. La surface dâ€™attaque est maÃ®trisÃ©e par la stack Zero Trust.
- **SynthÃ¨se** : HÃ©berger Omni en local rÃ©duit un peu le risque thÃ©orique, mais crÃ©e un risque opÃ©rationnel rÃ©el (perte de contrÃ´le quand le serveur est off). HÃ©berger Omni sur OCI avec Tunnel + SSO offre un bon compromis : disponibilitÃ© permanente pour lâ€™administration, risque limitÃ© par les contrÃ´les dâ€™accÃ¨s.

**RÃ©fÃ©rence** : approche similaire Ã  [Omni et KubeVirt (a cup of coffee)](https://a-cup-of.coffee/blog/omni/) â€” Omni comme point central dâ€™administration, avec possibilitÃ© de clusters Ã©phÃ©mÃ¨res via KubeVirt.

---

### 2.4 GitOps: ArgoCD

**Decision**: ArgoCD for GitOps continuous deployment

**Rationale** (vs Flux):
- âœ… Better UI for multi-cluster management
- âœ… ApplicationSets for dynamic app discovery
- âœ… Sync waves for ordered deployments
- âœ… Web UI for visibility and debugging
- âœ… Strong multi-tenancy support

**Key Features**:
- **Sync Waves**: Ordered deployment (infrastructure â†’ core â†’ apps)
- **ApplicationSets**: Auto-discover apps from directory structure
- **Multi-Cluster**: Deploy to Dev/Prod/Cloud from single ArgoCD
- **Self-Management**: ArgoCD manages its own configuration

**Alternative**: Flux CD (used by Cozystack, also excellent)

---

### 2.5 Dev/Prod Workflow (DEV Ã©phÃ©mÃ¨re via KubeVirt + Omni)

**Decision**: Pas de cluster DEV permanent sur Proxmox. Cluster DEV Ã©phÃ©mÃ¨re crÃ©Ã© Ã  la volÃ©e par la CI via KubeVirt + Omni, puis dÃ©truit aprÃ¨s les tests (ou aprÃ¨s MEP si pas dâ€™erreur).

**Rationale** (inspirÃ© de [Omni et KubeVirt - a cup of coffee](https://a-cup-of.coffee/blog/omni/)) :
- Pas besoin dâ€™une VM DEV qui tourne en permanence sur Proxmox (Ã©conomie de RAM/CPU).
- La CI crÃ©e un cluster DEV dans le cluster CLOUD (KubeVirt), dÃ©ploie les manifests, lance les tests, puis dÃ©truit le cluster.
- Si les tests passent et quâ€™on merge (MEP), le cluster Ã©phÃ©mÃ¨re est dÃ©truit ; la promotion vers PROD se fait via ArgoCD comme aujourdâ€™hui.

**Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Git Push    â”‚â”€â”€â”€â”€â–¶â”‚  CI Pipeline â”‚â”€â”€â”€â”€â–¶â”‚  Omni: create DEV        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  cluster (KubeVirt)      â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚  Deploy +    â”‚
                                          â”‚  Test on DEV â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                       â”‚                       â”‚
                          â–¼                       â–¼                       â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  Tests fail  â”‚         â”‚  MEP (merge) â”‚       â”‚  No MEP yet  â”‚
                  â”‚  â†’ Destroy   â”‚         â”‚  â†’ Destroy   â”‚       â”‚  â†’ Keep or   â”‚
                  â”‚  DEV cluster â”‚         â”‚  DEV cluster â”‚       â”‚  destroy     â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                  â”‚
                                                  â–¼
                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                          â”‚ PROD Deploy  â”‚
                                          â”‚ (ArgoCD)     â”‚
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:
```
kubernetes/
â”œâ”€â”€ base/                    # Shared base manifests
â”‚   â””â”€â”€ [service]/
â”œâ”€â”€ overlays/
â”‚   â”œâ”€â”€ dev/                 # Dev-specific patches
â”‚   â”‚   â””â”€â”€ [service]/
â”‚   â””â”€â”€ prod/                # Prod-specific patches
â”‚       â””â”€â”€ [service]/
â””â”€â”€ clusters/
    â”œâ”€â”€ dev/                 # Dev cluster kustomizations
    â”œâ”€â”€ prod/                # Prod cluster kustomizations
    â””â”€â”€ cloud/               # Oracle Cloud cluster
```

**Promotion Strategies**:
1. **Manual**: PR from dev branch to prod branch
2. **Automatic**: Time-based promotion (stable for X hours â†’ auto-promote)
3. **GitOps**: Different branches for dev/prod

---

### 2.6 Networking Architecture

**Decision**: Cilium CNI + Gateway API

**Rationale**:
- âœ… eBPF-based networking (high performance)
- âœ… Gateway API support (modern ingress)
- âœ… Built-in WireGuard encryption
- âœ… Hubble observability
- âœ… Network policies

**Components**:
- **Cilium**: CNI, kube-proxy replacement, network policies
- **MetalLB**: Load balancer for bare metal
- **Gateway API**: Modern ingress (Traefik or Cilium Gateway)
- **external-dns**: Automatic DNS management (Cloudflare)
- **cert-manager**: TLS certificate automation

---

### 2.7 Storage Architecture

**Decision**: Multi-tier storage strategy

**Proxmox (Homelab)**:
```
Storage Tiers:
â”œâ”€â”€ ZFS (local-zfs)          # High-performance, data integrity
â”‚   â”œâ”€â”€ VM disks
â”‚   â””â”€â”€ Container storage
â”œâ”€â”€ Longhorn/OpenEBS         # Kubernetes distributed storage
â”‚   â””â”€â”€ Replicated PVCs
â””â”€â”€ NFS                      # Shared storage for media
    â”œâ”€â”€ Films/Series
    â”œâ”€â”€ Music
    â””â”€â”€ Audiobooks
```

**Oracle Cloud**:
```
Storage Tiers:
â”œâ”€â”€ Block Storage (139GB)    # Boot + ephemeral
â””â”€â”€ NFS via VPN              # Access to homelab storage
```

**Kubernetes Storage Classes**:
- `local-path`: Local node storage (dev, non-critical)
- `longhorn`: Replicated storage (prod, critical)
- `nfs-media`: NFS for media files (12TB)

---

### 2.8 Security Architecture (2-Tier Authentication)

**Authentication Strategy**:

| Tier | Services | Authentication | Rationale |
|------|----------|----------------|-----------|
| **Tier 1 - Private Data** | Nextcloud, Immich, Vaultwarden, BaÃ¯kal, n8n | Authentik SSO + oauth2-proxy | Sensitive data requires centralized identity |
| **Tier 2 - Media/Public** | Navidrome, Komga, Romm, Audiobookshelf, Mealie, Invidious | App-native auth + Cloudflare | Multi-user apps with built-in user management |

**Workload Proxy (Omni)** â€” option pour lâ€™authentification des services :

Omni propose un **Workload Proxy** qui expose des services Kubernetes via lâ€™interface web Omni, avec authentification gÃ©rÃ©e par Omni (OIDC). IntÃ©rÃªt pour le homelab :

- **Un seul point dâ€™auth** : pas besoin dâ€™un LoadBalancer par service ni dâ€™un Ingress + certificat SSL dÃ©diÃ© pour chaque app.
- **RÃ©vocation immÃ©diate** : comme Omni agit en proxy devant lâ€™API et les services, rÃ©voquer un utilisateur dans Omni coupe lâ€™accÃ¨s tout de suite (vs token OIDC qui reste valide jusquâ€™Ã  expiration).
- **Exposition contrÃ´lÃ©e** : les services sont accessibles uniquement via le rÃ©seau Wireguard dâ€™Omni, pas directement sur Internet.

Pour activer le Workload Proxy sur un cluster Omni : `features.enableWorkloadProxy: true` dans la config du cluster. Les services Ã  exposer reÃ§oivent une annotation (ex. `omni-kube-service-exposer.sidero.dev/port`, `.../label`). IdÃ©al pour des outils internes (Grafana, ArgoCD, dashboards) quâ€™on veut protÃ©ger par lâ€™identitÃ© Omni/Authentik sans dÃ©ployer oauth2-proxy devant chaque service.

**Choix dâ€™usage** : Workload Proxy peut complÃ©ter ou remplacer oauth2-proxy pour certains services (dashboards, outils admin). Pour les apps Â« mÃ©tier Â» (Nextcloud, Vaultwarden, etc.), on garde Authentik SSO + oauth2-proxy ; pour des UIs lÃ©gÃ¨res ou internes, le Workload Proxy est une option intÃ©ressante. Voir [Omni Workload Proxy (a cup of coffee)](https://a-cup-of.coffee/blog/omni/#workload-proxy).

**IdP retenu : Authentik**.  
Omni sert de proxy (UI, kubeconfig, talosctl) ; Authentik sâ€™y connecte en **SAML** ([Integrate Authentik with Omni](https://integrations.goauthentik.io/infrastructure/omni/)). SSO pour les apps (Nextcloud, Vaultwarden, etc.) via OIDC + oauth2-proxy ou proxy Authentik.

**Flux utilisateur (invitation-only, trafic via Cloudflare)** :  
- **Onboarding par invitation uniquement** : self-registration **dÃ©sactivÃ©e**. Lâ€™admin crÃ©e une invitation (UI Authentik ou API) et envoie le lien Ã  lâ€™utilisateur ; le flow dâ€™enrollment nâ€™est accessible quâ€™avec un token dâ€™invitation. Voir `decision-invitation-only-et-acces-cloudflare.md`.  
- **Pas dâ€™accÃ¨s sans groupes** : tant quâ€™il nâ€™est pas dans les groupes autorisÃ©s, lâ€™utilisateur nâ€™a accÃ¨s Ã  aucune application ; les policies Authentik refusent lâ€™accÃ¨s.  
- **Validation / groupes** : aprÃ¨s enrollment, lâ€™admin ajoute lâ€™utilisateur aux groupes autorisÃ©s (ex. `family-validated`, `family-app-nextcloud`) ; optionnellement un job CI manuel peut appeler lâ€™API Authentik pour ajouter aux groupes ou dÃ©clencher le provisionnement.  
- **AprÃ¨s validation** : accÃ¨s aux apps (Nextcloud, Navidrome, etc.) selon les groupes ; la CI peut crÃ©er les comptes dans chaque app (webhook Authentik ou job manuel).  
- **Trafic utilisateur via Cloudflare** : toutes les connexions utilisateurs (auth, portail Authentik, apps protÃ©gÃ©es) **passent par Cloudflare** (Tunnel) ; pas dâ€™accÃ¨s direct Ã  lâ€™origine pour les utilisateurs finaux. RÃ¨gles WAF/config possibles pour renforcer.  
- **Design formalisÃ©** : flux, listes apps, CI, service accounts â†’ `session-travail-authentik.md` Â§6 ; invitation-only et Cloudflare â†’ `decision-invitation-only-et-acces-cloudflare.md`.

**Applications dâ€™administration non exposÃ©es** :  
Les apps dâ€™administration (Authentik Admin, Omni UI, ArgoCD, Grafana admin, Prometheus, etc.) **ne sont pas** exposÃ©es aux utilisateurs finaux : pas de lien dans le portail Authentik pour les groupes Â« famille Â» ; accÃ¨s rÃ©servÃ© aux admins (groupe dÃ©diÃ©) ou par URL/accÃ¨s restreint (IP, VPN). Les policies Authentik et bindings dâ€™applications distinguent Â« apps famille Â» (visibles) et Â« apps admin Â» (cachÃ©es / rÃ©servÃ©es).

**Service accounts** :  
Authentik fournit des **service accounts** (utilisateurs de type `service_account`) pour les connexions machine-to-machine (CI, ArgoCD, scripts, n8n, etc.). Droits granulaires par compte ; gestion en **Terraform** (provider goauthentik/authentik) pour `terraform apply`. Secrets stockÃ©s dans un secret manager (Bitwarden / Vault), pas en clair dans le repo.

**RBAC & onboarding** : DÃ©tail et options (catalogue dâ€™apps, webhook CI) : voir `session-travail-authentik.md` Â§6 (dÃ©cisions prises).

**Security Layers**:
1. **Identity**: Authentik SSO (OIDC/SAML) for private services ; validation manuelle avant accÃ¨s ; apps admin non exposÃ©es ; service accounts pour M2M
2. **Access**: oauth2-proxy for Tier 1, app-native auth for Tier 2
3. **RBAC / onboarding**: invitation-only (pas de self-registration) + catalogue apps (hors admin) + CI provisionnement (voir recherche RBAC)
4. **Network**: Cilium network policies, Cloudflare Tunnel (no open ports)
5. **DDoS/WAF**: Cloudflare protection for all public services
6. **Secrets**: External Secrets Operator + Bitwarden (â†’ Vault later)
7. **Images**: Trivy + Grype scanning in CI/CD
8. **OS**: Talos immutable OS (no SSH)

**Secrets Management**:
- **Phase 1**: Bitwarden Secrets (simple, existing infra)
- **Phase 2**: HashiCorp Vault (self-hosted, advanced)

**Zero Trust Architecture**:
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTERNET                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                    â”‚                    â”‚
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cloudflare      â”‚  â”‚ Twingate        â”‚  â”‚ Authentik SSO   â”‚
â”‚ Tunnel + WAF    â”‚  â”‚ (Zero Trust)    â”‚  â”‚ + oauth2-proxy  â”‚
â”‚ (All Apps)      â”‚  â”‚ (NFS Access)    â”‚  â”‚ (Tier 1 only)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                    â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Oracle Cloud / Homelab       â”‚
              â”‚  (No open ports to internet)  â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits**:
- No port forwarding on home router
- Per-service access control (not VPN = full network)
- Cloudflare WAF + DDoS protection for all services
- Authentik SSO for sensitive data only (reduced complexity)
- Individual user accounts in media apps (playlists, progress tracking)

---

### 2.9 Monitoring & Observability

**Stack**:
- **Metrics**: Prometheus
- **Logs**: Loki + Alloy (Grafana Agent)
- **Dashboards**: Grafana (admin dashboard on Homelab)
- **Alerting**: Alertmanager â†’ ntfy + Telegram â†’ Mobile push
- **User Dashboard**: Glance (Oracle Cloud) for family

**Alerting Flow**:
```
Prometheus â†’ Alertmanager â†’ ntfy (push) + Telegram bot
                              â†“
                         Mobile notifications
```

**Per-Cluster Monitoring**:
- Homelab PROD: Full stack (Prometheus, Grafana, Loki, Alertmanager)
- Oracle Cloud: Lightweight (metrics forwarded to Homelab Grafana)

---

### 2.10 Backup Strategy

**3-2-1 Rule**:
- 3 copies of data
- 2 different storage types
- 1 offsite

**Implementation**:
```
Backup Targets:
â”œâ”€â”€ Local (ZFS snapshots)     # Immediate recovery
â”œâ”€â”€ NAS (rsync/restic)        # Local backup
â””â”€â”€ Cloud (OVH Object Storage) # Offsite (3TB free)
    â”œâ”€â”€ Critical configs
    â”œâ”€â”€ Databases
    â””â”€â”€ Photos (Immich)
```

**Tools**:
- **Velero**: Kubernetes backup (PVs, configs)
- **Restic/Volsync**: File-level backup to S3
- **ZFS snapshots**: Local point-in-time recovery

---

## 3. Cluster Topology

### 3.1 DEV Cluster (Ã©phÃ©mÃ¨re, KubeVirt sur CLOUD)

**Purpose**: Validation CI uniquement. Pas de cluster DEV permanent sur Proxmox.

**Design Philosophy** (inspirÃ© de [Omni et KubeVirt - a cup of coffee](https://a-cup-of.coffee/blog/omni/)) :
- Le cluster CLOUD (Oracle Cloud) hÃ©berge KubeVirt.
- Omni dispose dâ€™un **Infrastructure Provider KubeVirt** : Ã  la demande, Omni crÃ©e des VMs Talos dans ce cluster Kubernetes.
- La CI dÃ©clenche la crÃ©ation dâ€™un cluster DEV via un template Omni (MachineClass KubeVirt), dÃ©ploie les manifests (ArgoCD ou kubectl), lance les tests, puis dÃ©truit le cluster (ou le garde jusquâ€™Ã  MEP puis destruction).

**Ressources (Ã©phÃ©mÃ¨res)** :
| RÃ´le | Taille | Notes |
|------|--------|-------|
| Control Plane | 1 node | CrÃ©Ã© par Omni KubeVirt provider |
| Workers | 1 node | Idem, selon template |

**Flux typique** :
1. CI (GitHub Actions) : `omnictl cluster template sync -f omni/clusters/dev-ephemeral.yaml` (ou API Omni).
2. Attente que le cluster soit Ready.
3. RÃ©cupÃ©ration du kubeconfig : `omnictl kubeconfig --cluster dev-ephemeral-<run-id>`.
4. DÃ©ploiement des manifests + tests (e.g. e2e, smoke).
5. Si MEP ou fin de run : destruction du cluster via Omni (ou TTL automatique si implÃ©mentÃ©).

**PrÃ©requis techniques** :
- Cluster CLOUD avec KubeVirt + CDI + storage (ex. LocalPathProvisioner ou autre CSI).
- Omni configurÃ© avec un ServiceAccount Â« InfraProvider Â» et le provider KubeVirt (`omni-infra-provider-kubevirt`) pointant vers le kubeconfig du cluster CLOUD.
- MachineClass Omni pour KubeVirt (ex. `hetzner` dans lâ€™article, ici par ex. `oci-kubevirt`).
- Patch Talos pour Ã©viter le chevauchement de CIDR (pod/service) entre lâ€™hÃ´te KubeVirt et les VMs Talos (voir article).

**Deployed Services (sur le cluster Ã©phÃ©mÃ¨re)** :
- MÃªme base que Prod (validates compatibility), overlays dev (replicas=1, ressources rÃ©duites).
- Pas de donnÃ©es persistantes critiques ; tests non destructifs ou fixtures.

---

### 3.2 PROD Cluster (Proxmox)

**Purpose**: Stable production services, local access

**Resources**:
| Node | Role | vCPU | RAM | Storage |
|------|------|------|-----|---------|
| talos-prod-cp | Control Plane | 2 | 4GB | 50GB |
| talos-prod-worker-1 | Worker | 6 | 12GB | 200GB |

**Total**: 8 vCPU, 16GB RAM

**Deployed Services**:
- AdGuard Home (DNS + ad blocking)
- Home Assistant (domotique)
- Komga (comics)
- Romm (ROMs)
- Audiobookshelf (audiobooks)
- Prometheus + Grafana + Loki (monitoring)
- ntfy (push notifications)

**Gaming VM** (Proxmox direct, Phase 3: KubeVirt):
- Windows Gaming VM (32GB RAM, GPU passthrough)
- On-demand streaming style GeForce Now (future)

---

### 3.3 CLOUD Cluster (Oracle Cloud)

**Purpose**: Family-shared services, external access, Omni management, **et hÃ´te KubeVirt pour clusters DEV Ã©phÃ©mÃ¨res** (Omni Infrastructure Provider KubeVirt).

**Resources** (Always Free Tier - 24GB RAM, 4 OCPUs, 200GB storage):
| Node | Role | OCPU | RAM | Storage |
|------|------|------|-----|---------|
| oci-mgmt | Omni + Authentik + Infra | 1 | 6GB | 50GB |
| oci-node-1 | Control Plane + Worker | 2 | 12GB | 64GB |
| oci-node-2 | Worker | 1 | 6GB | 75GB |

**Total**: 4 OCPUs, 24GB RAM, 189GB storage

**Management Node (oci-mgmt) - NOT in Kubernetes**:
- **Omni** (self-hosted): Talos cluster management (recommandation : hÃ©bergÃ© sur OCI pour disponibilitÃ© mÃªme quand le serveur Homelab est Ã©teint â€” voir Â§ 2.3.1).
- **Authentik**: SSO/Identity provider for all services
- **Cloudflare Tunnel**: Zero-trust exposure (no open ports)
- **PostgreSQL**: Database for Omni + Authentik

**KubeVirt sur le cluster CLOUD** (pour DEV Ã©phÃ©mÃ¨re) :
- KubeVirt + CDI installÃ©s sur le cluster CLOUD (oci-node-1, oci-node-2).
- Omni Infrastructure Provider KubeVirt : ServiceAccount Omni Â« InfraProvider Â», kubeconfig pointant vers le cluster CLOUD, provider `omni-infra-provider-kubevirt` (ex. `ghcr.io/siderolabs/omni-infra-provider-kubevirt`) qui crÃ©e les VMs Talos Ã  la demande.
- MachineClass Omni (ex. `oci-kubevirt`) pour les clusters Ã©phÃ©mÃ¨res.
- Storage : LocalPathProvisioner ou autre CSI pour les disques des VMs (voir [article a cup of coffee](https://a-cup-of.coffee/blog/omni/) pour LocalPathProvisioner + patch Talos).

**Deployed Services on Kubernetes Cluster**:

**Namespace: media**
- **Comet** (Real-Debrid addon for Stremio clients) âš ï¸ CRITICAL
- Navidrome (music streaming) - storage via NFS to Homelab
- Lidarr (music automation) - storage via NFS to Homelab

> **âš ï¸ COMET CRITICAL REQUIREMENTS**:
> - **Static IP**: Real-Debrid requires consistent IP (bans accounts for IP changes)
> - **Maximum Uptime**: Family depends on this for streaming
> - **Oracle Cloud Fit**: Static public IP + enterprise uptime + Always Free
> - Stremio is a desktop/mobile client - users install locally and connect to Comet

**Namespace: critical** (Authentik SSO)
- Vaultwarden (passwords)
- BaÃ¯kal (CalDAV/CardDAV)
- Twingate Connector (Zero Trust VPN to homelab for NFS)
- oauth2-proxy (SSO enforcement)

**Namespace: collaborative** (Authentik SSO)
- Nextcloud (cloud storage) - storage via NFS to Homelab

**Namespace: dashboard**
- Glance (family dashboard/homepage)

**Namespace: optional** (Phase 2 - deploy as resources allow)
- Immich (photos) - Authentik SSO, storage via NFS to Homelab
- n8n (automation workflows) - Authentik SSO
- Mealie (recipes) - app-native auth
- Invidious (YouTube frontend) - app-native auth

---

## 4. Implementation Patterns

### 4.1 Repository Structure

```
homelab/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                    # Lint, validate, security scan
â”‚       â”œâ”€â”€ deploy-dev.yml            # Deploy to DEV on push
â”‚       â”œâ”€â”€ promote-prod.yml          # Promote DEV â†’ PROD
â”‚       â”œâ”€â”€ renovate.yml              # Dependency updates
â”‚       â””â”€â”€ trivy-scan.yml            # Image security scanning
â”‚
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ proxmox/
â”‚   â”‚   â”œâ”€â”€ main.tf                   # Provider configuration
â”‚   â”‚   â”œâ”€â”€ variables.tf              # Input variables
â”‚   â”‚   â”œâ”€â”€ outputs.tf                # Output values
â”‚   â”‚   â”œâ”€â”€ talos-vms.tf              # Talos VM definitions
â”‚   â”‚   â”œâ”€â”€ gaming-vms.tf             # Gaming VM definitions
â”‚   â”‚   â””â”€â”€ network.tf                # Network configuration
â”‚   â””â”€â”€ oracle-cloud/
â”‚       â”œâ”€â”€ main.tf                   # OCI provider
â”‚       â”œâ”€â”€ compute.tf                # VM instances (mgmt + k8s nodes)
â”‚       â””â”€â”€ network.tf                # VCN, subnets
â”‚
â”œâ”€â”€ docker/                            # Docker Compose for management VM
â”‚   â””â”€â”€ oci-mgmt/
â”‚       â”œâ”€â”€ docker-compose.yml        # Omni + Authentik + Cloudflare
â”‚       â”œâ”€â”€ omni/
â”‚       â”‚   â””â”€â”€ config.yaml           # Omni configuration
â”‚       â”œâ”€â”€ authentik/
â”‚       â”‚   â””â”€â”€ config/               # Flows, providers, policies (ou Terraform)
â”‚       â”œâ”€â”€ cloudflared/
â”‚       â”‚   â””â”€â”€ config.yml            # Tunnel configuration
â”‚       â””â”€â”€ nginx/
â”‚           â””â”€â”€ nginx.conf            # Reverse proxy config
â”‚
â”œâ”€â”€ omni/
â”‚   â”œâ”€â”€ clusters/
â”‚   â”‚   â”œâ”€â”€ dev-ephemeral.yaml        # DEV Ã©phÃ©mÃ¨re (KubeVirt, CI)
â”‚   â”‚   â”œâ”€â”€ prod.yaml                 # PROD cluster template
â”‚   â”‚   â””â”€â”€ cloud.yaml                # CLOUD cluster template
â”‚   â”œâ”€â”€ machine-classes/
â”‚   â”‚   â”œâ”€â”€ control-plane.yaml        # CP machine class
â”‚   â”‚   â”œâ”€â”€ worker.yaml               # Standard worker
â”‚   â”‚   â””â”€â”€ gpu-worker.yaml           # GPU-enabled worker
â”‚   â””â”€â”€ patches/
â”‚       â””â”€â”€ cilium-config.yaml        # Cilium configuration
â”‚
â”œâ”€â”€ kubernetes/
â”‚   â”œâ”€â”€ base/                         # Base manifests (shared)
â”‚   â”‚   â”œâ”€â”€ argocd/                   # ArgoCD configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ install.yaml          # ArgoCD installation
â”‚   â”‚   â”‚   â”œâ”€â”€ root.yaml             # Root application
â”‚   â”‚   â”‚   â””â”€â”€ apps/                 # ApplicationSets
â”‚   â”‚   â”‚       â”œâ”€â”€ infra.yaml        # Infrastructure apps (wave 0-1)
â”‚   â”‚   â”‚       â”œâ”€â”€ core.yaml         # Core services (wave 2)
â”‚   â”‚   â”‚       â”œâ”€â”€ monitoring.yaml   # Monitoring (wave 3)
â”‚   â”‚   â”‚       â””â”€â”€ apps.yaml         # User applications (wave 4)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ infrastructure/           # Wave 0-1
â”‚   â”‚   â”‚   â”œâ”€â”€ cilium/
â”‚   â”‚   â”‚   â”œâ”€â”€ metallb/
â”‚   â”‚   â”‚   â”œâ”€â”€ cert-manager/
â”‚   â”‚   â”‚   â”œâ”€â”€ external-dns/
â”‚   â”‚   â”‚   â”œâ”€â”€ external-secrets/
â”‚   â”‚   â”‚   â”œâ”€â”€ longhorn/             # Prod storage
â”‚   â”‚   â”‚   â””â”€â”€ local-path/           # Dev storage (minimal)
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ security/                 # Wave 2
â”‚   â”‚   â”‚   â”œâ”€â”€ oauth2-proxy/         # SSO enforcement (Tier 1)
â”‚   â”‚   â”‚   â”œâ”€â”€ twingate/             # Zero Trust connector
â”‚   â”‚   â”‚   â”œâ”€â”€ vaultwarden/
â”‚   â”‚   â”‚   â””â”€â”€ baikal/
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ monitoring/               # Wave 3
â”‚   â”‚   â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”‚   â”œâ”€â”€ loki/
â”‚   â”‚   â”‚   â”œâ”€â”€ alloy/                # Grafana agent
â”‚   â”‚   â”‚   â”œâ”€â”€ alertmanager/
â”‚   â”‚   â”‚   â””â”€â”€ ntfy/                 # Push notifications
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ apps/                     # Wave 4
â”‚   â”‚       â”œâ”€â”€ media/
â”‚   â”‚       â”‚   â”œâ”€â”€ comet/            # Real-Debrid addon (CRITICAL)
â”‚   â”‚       â”‚   â”œâ”€â”€ navidrome/        # Oracle Cloud
â”‚   â”‚       â”‚   â””â”€â”€ lidarr/           # Oracle Cloud
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ home/                 # Homelab PROD only
â”‚   â”‚       â”‚   â”œâ”€â”€ adguard-home/     # DNS + ad blocking
â”‚   â”‚       â”‚   â”œâ”€â”€ home-assistant/
â”‚   â”‚       â”‚   â”œâ”€â”€ audiobookshelf/   # Moved from Cloud
â”‚   â”‚       â”‚   â”œâ”€â”€ komga/
â”‚   â”‚       â”‚   â””â”€â”€ romm/
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ collaborative/
â”‚   â”‚       â”‚   â””â”€â”€ nextcloud/
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ dashboard/
â”‚   â”‚       â”‚   â””â”€â”€ glance/           # Family dashboard
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ optional/             # Phase 2
â”‚   â”‚           â”œâ”€â”€ immich/
â”‚   â”‚           â”œâ”€â”€ n8n/
â”‚   â”‚           â”œâ”€â”€ mealie/
â”‚   â”‚           â””â”€â”€ invidious/
â”‚   â”‚
â”‚   â”œâ”€â”€ overlays/
â”‚   â”‚   â”œâ”€â”€ dev/                      # DEV-specific patches
â”‚   â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”‚   â””â”€â”€ patches/
â”‚   â”‚   â”œâ”€â”€ prod/                     # PROD-specific patches
â”‚   â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â”‚   â””â”€â”€ patches/
â”‚   â”‚   â””â”€â”€ cloud/                    # Oracle Cloud patches
â”‚   â”‚       â”œâ”€â”€ kustomization.yaml
â”‚   â”‚       â””â”€â”€ patches/
â”‚   â”‚
â”‚   â””â”€â”€ clusters/                     # Per-cluster configs
â”‚       â”œâ”€â”€ dev/
â”‚       â”‚   â””â”€â”€ kustomization.yaml    # Points to base + dev overlay
â”‚       â”œâ”€â”€ prod/
â”‚       â”‚   â””â”€â”€ kustomization.yaml    # Points to base + prod overlay
â”‚       â””â”€â”€ cloud/
â”‚           â””â”€â”€ kustomization.yaml    # Points to base + cloud overlay
â”‚
â”œâ”€â”€ ansible/
â”‚   â”œâ”€â”€ playbooks/
â”‚   â”‚   â”œâ”€â”€ proxmox-setup.yml         # Initial Proxmox config
â”‚   â”‚   â”œâ”€â”€ zfs-setup.yml             # ZFS pool configuration
â”‚   â”‚   â””â”€â”€ security-hardening.yml    # Security baseline
â”‚   â””â”€â”€ inventory/
â”‚       â””â”€â”€ hosts.yml
â”‚
â”œâ”€â”€ packer/                            # VM template building
â”‚   â””â”€â”€ talos/
â”‚       â”œâ”€â”€ talos.pkr.hcl             # Talos image template
â”‚       â””â”€â”€ variables.pkr.hcl
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ bootstrap-cluster.sh          # Initial cluster bootstrap
â”‚   â”œâ”€â”€ promote-to-prod.sh            # DEV â†’ PROD promotion
â”‚   â”œâ”€â”€ backup.sh                     # Backup automation
â”‚   â””â”€â”€ validate-manifests.sh         # Pre-commit validation
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ operations.md
â”‚   â”œâ”€â”€ troubleshooting.md
â”‚   â””â”€â”€ runbooks/
â”‚
â”œâ”€â”€ .taskfiles/                        # Taskfile definitions
â”‚   â”œâ”€â”€ Taskfile.terraform.yml
â”‚   â”œâ”€â”€ Taskfile.kubernetes.yml
â”‚   â””â”€â”€ Taskfile.backup.yml
â”‚
â”œâ”€â”€ Taskfile.yaml                      # Main taskfile
â”œâ”€â”€ renovate.json                      # Renovate configuration
â”œâ”€â”€ .sops.yaml                         # SOPS encryption config
â””â”€â”€ README.md
```

---

### 4.2 ArgoCD Sync Waves

**Wave Architecture** (inspired by mitchross/talos-argocd-proxmox):

```yaml
# Wave 0: Foundation (Networking & Secrets)
- Cilium
- MetalLB
- External Secrets Operator
- cert-manager

# Wave 1: Storage
- Longhorn / OpenEBS
- NFS provisioner

# Wave 2: Core Infrastructure
- external-dns
- Traefik / Gateway API
- Databases (PostgreSQL, Redis)

# Wave 3: Monitoring
- Prometheus
- Grafana
- Loki
- Alertmanager

# Wave 4: Applications
- User workloads
- Media services
- Collaborative tools
```

**Implementation**:
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: cilium
  annotations:
    argocd.argoproj.io/sync-wave: "0"
spec:
  # ...
```

---

### 4.3 Service Pattern Template

**Standard service structure**:
```
kubernetes/base/apps/[category]/[service-name]/
â”œâ”€â”€ deployment.yaml           # Deployment resource
â”œâ”€â”€ service.yaml              # Service resource
â”œâ”€â”€ configmap.yaml            # Non-sensitive config
â”œâ”€â”€ external-secret.yaml      # Secrets from Vault/Bitwarden
â”œâ”€â”€ pvc.yaml                  # Persistent storage
â”œâ”€â”€ ingress.yaml              # Ingress/HTTPRoute
â”œâ”€â”€ network-policy.yaml       # Network isolation
â”œâ”€â”€ servicemonitor.yaml       # Prometheus metrics
â””â”€â”€ kustomization.yaml        # Kustomize file
```

**Naming Conventions**:
- **Deployment**: `[service-name]`
- **Service**: `[service-name]`
- **ConfigMap**: `[service-name]-config`
- **Secret**: `[service-name]-secrets`
- **PVC**: `[service-name]-data`
- **Ingress**: `[service-name]`

**Mandatory Labels**:
```yaml
labels:
  app.kubernetes.io/name: [service-name]
  app.kubernetes.io/instance: [service-name]
  app.kubernetes.io/component: [component]
  app.kubernetes.io/part-of: homelab
  app.kubernetes.io/managed-by: argocd
```

---

### 4.4 Environment Overlays

**Base â†’ Overlay Pattern**:

```yaml
# kubernetes/overlays/dev/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: default
resources:
  - ../../base/apps/media/navidrome
patches:
  - patch: |
      - op: replace
        path: /spec/replicas
        value: 1
    target:
      kind: Deployment
      name: navidrome
  - path: patches/resources-dev.yaml
```

```yaml
# kubernetes/overlays/prod/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: default
resources:
  - ../../base/apps/media/navidrome
patches:
  - path: patches/resources-prod.yaml
  - path: patches/replicas-prod.yaml
```

---

### 4.5 Terraform Patterns for Proxmox

**Talos VM Definition**:
```hcl
# terraform/proxmox/talos-vms.tf
resource "proxmox_virtual_environment_vm" "talos_prod_cp" {
  name      = "talos-prod-cp"
  node_name = "proxmox"
  
  cpu {
    cores = 2
    type  = "host"
  }
  
  memory {
    dedicated = 4096
  }
  
  disk {
    datastore_id = "local-zfs"
    file_id      = proxmox_virtual_environment_download_file.talos_iso.id
    interface    = "virtio0"
    size         = 50
  }
  
  network_device {
    bridge = "vmbr0"
  }
  
  operating_system {
    type = "l26"  # Linux 2.6+ kernel
  }
  
  tags = ["talos", "kubernetes", "prod", "control-plane"]
}
```

---

### 4.6 Omni Cluster Templates

**Cluster Definition** (inspired by qjoly/GitOps):
```yaml
# omni/clusters/prod.yaml
kind: Cluster
metadata:
  name: prod
  labels:
    environment: production
spec:
  talosVersion: 1.9.0
  kubernetesVersion: 1.32.0
  
  controlPlane:
    machineClass: control-plane
    replicas: 1
    patches:
      - |
        machine:
          network:
            hostname: talos-prod-cp
          install:
            disk: /dev/vda
            
  workers:
    - machineClass: worker
      replicas: 1
      patches:
        - |
          machine:
            network:
              hostname: talos-prod-worker-1
              
    - machineClass: gpu-worker
      replicas: 1
      patches:
        - |
          machine:
            network:
              hostname: talos-prod-gpu
          kubelet:
            extraArgs:
              feature-gates: DevicePlugins=true
```

---

## 5. Services Catalog

### 5.0 Management Services (Oracle Cloud - NOT in K8s)

| Service | RAM | Description | Priority | Source |
|---------|-----|-------------|----------|--------|
| **Omni** | 1GB | Talos cluster management | ğŸ”´ Critical | Sidero Labs |
| **Authentik** | 1GB | SSO/Identity provider (OIDC) | ğŸ”´ Critical | [Various repos] |
| **PostgreSQL** | 512MB | Database for Omni + Authentik | ğŸ”´ Critical | - |
| **Cloudflare Tunnel** | 128MB | Zero-trust ingress (no open ports) | ğŸ”´ Critical | [qjoly, Mafyuh] |
| **Nginx** | 128MB | Reverse proxy + TLS termination | ğŸ”´ Critical | - |

**Total**: ~3GB RAM (runs on oci-mgmt VM, separate from K8s cluster)

**Why Authentik?** (inspired by multiple homelab repos):
- Single Sign-On for all services (Nextcloud, Grafana, ArgoCD, etc.) ; intÃ©gration Omni ([Integrate with Omni](https://integrations.goauthentik.io/infrastructure/omni/))
- OIDC/SAML, groupes, policies ; **validation manuelle** avant accÃ¨s aux apps (pas dâ€™accÃ¨s direct aprÃ¨s inscription)
- **Apps dâ€™administration non exposÃ©es** aux utilisateurs finaux (policies / bindings)
- **Service accounts** pour CI, ArgoCD, scripts ; Terraform (goauthentik/authentik) pour IaC
- Webhooks (Notification Transports) pour dÃ©clencher la CI (provisionnement)

---

### 5.1 Media Services (Oracle Cloud K8s)

| Service | RAM | Description | Priority | Auth | Storage |
|---------|-----|-------------|----------|------|---------|
| **Comet** | 256MB | Real-Debrid addon for Stremio | ğŸ”´ Critical | - | - |
| **Navidrome** | 512MB | Music streaming (Subsonic compatible) | ğŸ”´ Critical | App-native | NFS â†’ Homelab |
| **Lidarr** | 512MB | Music library automation | ğŸŸ¡ Important | App-native | NFS â†’ Homelab |

**Total**: ~1.3GB RAM

> **Stremio**: NOT hosted - it's a client app users install on their devices
> **Audiobookshelf**: Moved to Homelab PROD (local access, no need for cloud)

---

### 5.2 Critical Services (Oracle Cloud K8s)

| Service | RAM | Description | Priority | Auth |
|---------|-----|-------------|----------|------|
| **Vaultwarden** | 256MB | Password manager | ğŸ”´ Critical | Authentik SSO |
| **BaÃ¯kal** | 256MB | CalDAV/CardDAV | ğŸ”´ Critical | Authentik SSO |
| **Twingate Connector** | 128MB | Zero Trust VPN to homelab (NFS) | ğŸ”´ Critical | - |
| **oauth2-proxy** | 128MB | SSO enforcement for Tier 1 services | ğŸ”´ Critical | - |

**Total**: ~768MB RAM

**Why Twingate over WireGuard?**:
- Zero Trust architecture (no open ports on homelab)
- Per-service access control (not full network access)
- Works through NAT without port forwarding
- Free tier: 5 users (perfect for family)
- Used for NFS access from Oracle Cloud to Homelab storage

---

### 5.3 Collaborative Services (Oracle Cloud K8s)

| Service | RAM | Description | Priority | Auth | Storage |
|---------|-----|-------------|----------|------|---------|
| **Nextcloud** | 2GB | Cloud storage + collaboration | ğŸ”´ Critical | Authentik SSO | NFS â†’ Homelab |

**Total**: ~2GB RAM

> **Removed**: Gitea (GitHub sufficient), Actual Budget, La Suite (not needed)

---

### 5.4 Optional/Automation Services (Oracle Cloud K8s) - Phase 2

| Service | RAM | Description | Priority | Auth | Storage |
|---------|-----|-------------|----------|------|---------|
| **Glance** | 256MB | Family dashboard/homepage | ğŸŸ¡ Important | App-native | - |
| **Immich** | 2GB | Photo management | ğŸŸ¢ Phase 2 | Authentik SSO | NFS â†’ Homelab |
| **n8n** | 512MB | Workflow automation | ğŸŸ¢ Phase 2 | Authentik SSO | - |
| **Mealie** | 512MB | Recipe management | ğŸŸ¢ Phase 2 | App-native | - |
| **Invidious** | 1GB | YouTube frontend (privacy) | ğŸŸ¢ Phase 2 | App-native | - |

**Total**: ~4.3GB RAM (deploy after MVP when infra is stable)

**Why n8n?**:
- Automate workflows between services
- Self-hosted alternative to Zapier/Make
- Integrates with 400+ apps
- Useful for: backup notifications, service health checks, family alerts

---

### 5.5 Home Services (Homelab PROD)

| Service | RAM | Description | Priority | Auth |
|---------|-----|-------------|----------|------|
| **AdGuard Home** | 256MB | DNS + ad blocking (DoH/DoT native) | ğŸ”´ Critical | Local |
| **Home Assistant** | 2GB | Home automation | ğŸ”´ Critical | Local |
| **Audiobookshelf** | 1GB | Audiobook management/streaming | ğŸŸ¡ Important | App-native |
| **Komga** | 2GB | Comics/manga server | ğŸŸ¡ Important | App-native |
| **Romm** | 1GB | ROM management | ğŸŸ¡ Important | App-native |

**Total**: ~6.3GB RAM

> **Removed**: Pi-hole (replaced by AdGuard Home), Uptime Kuma (Alertmanager sufficient), Frigate (deferred), Homaar (Glance on Oracle Cloud)
> **Why AdGuard Home over Pi-hole?**: Modern UI, native DoH/DoT, lighter RAM, simpler config

---

### 5.6 Monitoring (Homelab PROD)

| Service | RAM | Description | Priority |
|---------|-----|-------------|----------|
| **Prometheus** | 1GB | Metrics collection | ğŸ”´ Critical |
| **Grafana** | 512MB | Admin dashboards & visualization | ğŸ”´ Critical |
| **Loki** | 512MB | Log aggregation | ğŸŸ¡ Important |
| **Alertmanager** | 256MB | Alert routing to ntfy + Telegram | ğŸŸ¡ Important |
| **Alloy** | 256MB | Grafana agent for logs/metrics | ğŸŸ¡ Important |
| **ntfy** | 128MB | Push notifications | ğŸŸ¡ Important |

**Total**: ~2.7GB RAM

**Alerting Channels**:
- **ntfy**: Mobile push notifications (self-hosted)
- **Telegram**: Bot for critical alerts

> **Removed**: Wazuh SIEM (overkill for homelab - Talos immutable OS + Cloudflare + Authentik provide sufficient security)

---

### 5.7 CI/CD Security Tools (GitHub Actions)

| Service | RAM | Location | Description | Source |
|---------|-----|----------|-------------|--------|
| **Trivy** | - | CI/CD | Image & config scanning | [Mafyuh] |
| **Grype** | - | CI/CD | Vulnerability scanning | [Mafyuh] |
| **GitGuardian** | - | CI/CD | Secret detection | [Mafyuh] |
| **kubeval** | - | CI/CD | Kubernetes manifest validation | [mitchross] |
| **yamllint** | - | CI/CD | YAML linting | - |

> These run in GitHub Actions, not on cluster resources

---

### 5.8 Gaming (Homelab)

**MVP (Proxmox Direct)**:
| VM | RAM | vCPU | GPU | Storage |
|----|-----|------|-----|---------|
| **Windows Gaming** | 32GB | 8 | Yes (passthrough) | 1TB |

**Phase 3 (KubeVirt - Future)**:
- On-demand VM provisioning via Kubernetes API
- Game streaming style GeForce Now
- VM templates for instant startup
- Web interface to launch gaming sessions

> **Removed**: Linux VM (not needed)
> **Gaming VM is OFF most of the time** - only started for gaming sessions

---

## 6. CI/CD Pipeline

### 6.1 Workflow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Git Push   â”‚â”€â”€â”€â–¶â”‚  CI Pipeline â”‚â”€â”€â”€â–¶â”‚  Deploy DEV  â”‚â”€â”€â”€â–¶â”‚  Stability   â”‚
â”‚   (main)     â”‚    â”‚  (Validate)  â”‚    â”‚  (Auto)      â”‚    â”‚  Validation  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                    â”‚
                                                                    â–¼
                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                            â”‚  Promote to  â”‚
                                                            â”‚  PROD (Manualâ”‚
                                                            â”‚  or Auto)    â”‚
                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 GitHub Actions Workflows

**CI Pipeline** (`.github/workflows/ci.yml`) â€” inchangÃ© : validation manifests, lint, Trivy.

**Ephemeral DEV (create â†’ test â†’ destroy)** (`.github/workflows/deploy-dev-ephemeral.yml` ou extension de `deploy-dev.yml`):
```yaml
# IdÃ©e : crÃ©er cluster DEV via Omni (KubeVirt), dÃ©ployer, tester, dÃ©truire
jobs:
  ephemeral-dev:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Create ephemeral DEV cluster (Omni + KubeVirt)
        run: |
          omnictl cluster template sync -f omni/clusters/dev-ephemeral.yaml
          # ou appel API Omni pour crÃ©er cluster avec ID unique (e.g. $GITHUB_RUN_ID)
      - name: Wait for cluster Ready
        run: |
          omnictl get cluster dev-ephemeral-${{ github.run_id }} --wait
      - name: Get kubeconfig
        run: omnictl kubeconfig --cluster dev-ephemeral-${{ github.run_id }} > kubeconfig
      - name: Deploy and test
        run: |
          kubectl apply -f kubernetes/...  # ou ArgoCD sync
          # run e2e / smoke tests
      - name: Destroy ephemeral cluster (on success or failure)
        if: always()
        run: omnictl cluster delete dev-ephemeral-${{ github.run_id }}
```

**Promote to PROD** (`.github/workflows/promote-prod.yml`) : aprÃ¨s validation sur DEV Ã©phÃ©mÃ¨re (MEP sans erreur), promotion vers PROD via ArgoCD (inchangÃ©).

---

## 7. Renovate Configuration

**Automated dependency updates** (inspired by ahinko/home-ops):

```json5
// renovate.json
{
  "$schema": "https://docs.renovatebot.com/renovate-schema.json",
  "extends": [
    "config:recommended",
    "docker:enableMajor",
    ":semanticCommits",
    ":automergeDigest",
    ":automergeBranch"
  ],
  "kubernetes": {
    "fileMatch": ["kubernetes/.+\\.ya?ml$"]
  },
  "helm-values": {
    "fileMatch": ["kubernetes/.+\\.ya?ml$"]
  },
  "packageRules": [
    {
      "description": "Auto-merge minor/patch for trusted packages",
      "matchUpdateTypes": ["minor", "patch"],
      "matchPackageNames": ["ghcr.io/linuxserver/*"],
      "automerge": true
    },
    {
      "description": "Group Prometheus stack updates",
      "matchPackagePatterns": ["prometheus", "grafana", "alertmanager"],
      "groupName": "monitoring-stack"
    }
  ]
}
```

---

## 8. Resource Allocation Summary

### Homelab (64GB RAM)

| Component | RAM | Notes |
|-----------|-----|-------|
| **Proxmox Host** | 4GB | OS overhead |
| **ZFS ARC Cache** | 8GB | Performance |
| **DEV Cluster** | 0GB | Ã‰phÃ©mÃ¨re sur CLOUD (KubeVirt), pas de VM Proxmox dÃ©diÃ©e |
| **PROD Cluster** | 16GB | Production services |
| **Gaming VM** | 32GB | When active (OFF most of the time) |
| **Reserve** | 4GB | Buffer |

**PROD Cluster Breakdown** (~16GB):
| Service | RAM |
|---------|-----|
| K8s overhead | 2GB |
| AdGuard Home | 256MB |
| Home Assistant | 2GB |
| Audiobookshelf | 1GB |
| Komga | 2GB |
| Romm | 1GB |
| Prometheus | 1GB |
| Grafana | 512MB |
| Loki | 512MB |
| Alertmanager | 256MB |
| Alloy | 256MB |
| ntfy | 128MB |
| Reserve | ~5GB |

**Normal Operation**: ~24GB used (PROD active, pas de cluster DEV permanent, Gaming OFF)
**Testing Mode (CI)** : cluster DEV Ã©phÃ©mÃ¨re crÃ©Ã© sur CLOUD (KubeVirt), pas de RAM Proxmox dÃ©diÃ©e.
**Gaming Mode**: ~48GB used (Gaming VM + PROD)

---

### Oracle Cloud (24GB RAM, 4 OCPUs)

**Management VM (oci-mgmt)** - Docker, NOT Kubernetes:
| Component | RAM | Notes |
|-----------|-----|-------|
| **Omni** | 1GB | Cluster management |
| **Authentik** | 1GB | SSO/Identity |
| **PostgreSQL** | 512MB | Database |
| **Cloudflare Tunnel** | 128MB | Zero-trust ingress |
| **Nginx** | 128MB | Reverse proxy |
| **Reserve** | 2GB | Buffer |
| **Total** | **~5GB** | 1 OCPU |

**Kubernetes Cluster (oci-node-1 + oci-node-2)** - MVP:
| Component | RAM | Notes |
|-----------|-----|-------|
| **K8s Overhead** | 2GB | Control plane, Cilium |
| **Media Services** | 1.3GB | Comet, Navidrome, Lidarr |
| **Critical Services** | 0.8GB | Vaultwarden, BaÃ¯kal, Twingate, oauth2-proxy |
| **Collaborative** | 2GB | Nextcloud |
| **Dashboard** | 256MB | Glance |
| **Reserve** | ~6GB | Buffer for Phase 2 services |
| **Total MVP** | **~12GB** | 3 OCPUs |

**Phase 2 Addition**:
| Component | RAM | Notes |
|-----------|-----|-------|
| **Immich** | 2GB | Photos |
| **n8n** | 512MB | Automation |
| **Mealie** | 512MB | Recipes |
| **Invidious** | 1GB | YouTube |
| **Total Phase 2** | **~4GB** | |

**Grand Total**: ~16-19GB RAM, 4 OCPUs âœ… Within Always Free limits

---

## 9. Implementation Roadmap

### Phase 1: Foundation
- [ ] Terraform Proxmox VMs (PROD nodes only ; pas de VM DEV dÃ©diÃ©e)
- [ ] Omni setup on Oracle Cloud (self-hosted, recommandation Â§ 2.3.1)
- [ ] PROD cluster bootstrap (Talos + Kubernetes)
- [ ] ArgoCD installation
- [ ] Cilium CNI deployment

### Phase 2: Core Infrastructure
- [ ] Storage (Longhorn/OpenEBS)
- [ ] cert-manager + external-dns
- [ ] External Secrets Operator + Bitwarden
- [ ] Monitoring stack (Prometheus, Grafana, Loki, ntfy)
- [ ] AdGuard Home (DNS)

### Phase 3: PROD Cluster + Oracle Cloud + DEV Ã©phÃ©mÃ¨re
- [ ] PROD cluster bootstrap
- [ ] Terraform OCI instances
- [ ] Oracle Cloud K8s cluster (CLOUD)
- [ ] KubeVirt + CDI + storage (LocalPathProvisioner ou CSI) sur CLOUD
- [ ] Omni Infrastructure Provider KubeVirt (MachineClass, ServiceAccount, provider container)
- [ ] Authentik SSO + Cloudflare Tunnel
- [ ] Twingate connector for NFS access
- [ ] CI/CD pipeline : workflow ephemeral DEV (create â†’ test â†’ destroy)

### Phase 4: Services MVP
- [ ] Critical: Vaultwarden, BaÃ¯kal
- [ ] Collaborative: Nextcloud
- [ ] Media: Comet, Navidrome, Lidarr
- [ ] Home: Home Assistant, Komga, Romm, Audiobookshelf
- [ ] Dashboard: Glance (family), Grafana (admin)

### Phase 5: Optional Services
- [ ] Immich (photos)
- [ ] n8n (automation)
- [ ] Mealie (recipes)
- [ ] Invidious (YouTube)

### Phase 6: Gaming & Advanced
- [ ] GPU passthrough setup
- [ ] Windows Gaming VM (Proxmox direct)
- [ ] KubeVirt integration (on-demand gaming)
- [ ] Backup automation

---

## 10. Validation Checklist

### Pre-Deployment
- [ ] All manifests pass `kubeval`
- [ ] No secrets in Git (checked by GitGuardian)
- [ ] Trivy scan passes (no critical vulnerabilities)
- [ ] YAML lint passes

### Post-Deployment
- [ ] All pods Running/Ready
- [ ] Ingress accessible
- [ ] TLS certificates valid
- [ ] Prometheus scraping metrics
- [ ] Alertmanager configured

---

**Document Status**: âœ… **VALIDATED & READY FOR IMPLEMENTATION**

Architecture v6.0 validated on 2026-01-31. Key changes from v5.0:
- **IdP : Authentik** (remplace Keycloak). IntÃ©gration Omni SAML, webhooks, service accounts, Terraform.
- **Flux utilisateur** : **invitation-only** (pas de self-registration) ; trafic utilisateur **via Cloudflare** ; **apps admin non exposÃ©es** aux utilisateurs finaux.
- **Design Authentik** : flux, listes apps, CI, service accounts â†’ `session-travail-authentik.md` Â§6 ; invitation-only et Cloudflare â†’ `decision-invitation-only-et-acces-cloudflare.md`.

This design provides:
- Pas de VM DEV 24/7 sur Proxmox (Ã©conomie de ressources).
- CI-driven ephemeral DEV (create â†’ test â†’ destroy) inspirÃ© de [Omni et KubeVirt - a cup of coffee](https://a-cup-of.coffee/blog/omni/).
- Workload Proxy comme option dâ€™auth pour services internes.
- Omni sur OCI pour disponibilitÃ© et administration Ã  distance.

Begin with Phase 1: Terraform Proxmox VMs and Omni setup ; puis KubeVirt + Omni provider sur CLOUD pour DEV Ã©phÃ©mÃ¨re.
